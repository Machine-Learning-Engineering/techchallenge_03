#!/usr/bin/env python3
"""
Persist√™ncia de dados do Coffee Sales no PostgreSQL
L√™ o arquivo CSV e armazena no banco de dados usando as credenciais do .env
"""

import sys
import logging
import os
from pathlib import Path
from typing import Optional, Dict, Any
import pandas as pd
import psycopg2
from sqlalchemy import create_engine, text, inspect
from dotenv import load_dotenv

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('persistencia.log')
    ]
)
logger = logging.getLogger(__name__)


class CoffeeSalesPersistencia:
    """Classe para gerenciar a persist√™ncia dos dados de vendas de caf√© no PostgreSQL"""
    
    def __init__(self):
        """
        Inicializa a conex√£o com PostgreSQL usando vari√°veis do .env
        """
        # Carregar vari√°veis de ambiente
        env_path = Path('../.env')
        if env_path.exists():
            load_dotenv(env_path)
            logger.info(f"Carregando .env de: {env_path.absolute()}")
        else:
            # Tentar no diret√≥rio atual
            load_dotenv()
            logger.info("Carregando .env do diret√≥rio atual")
        
        # Configura√ß√µes do PostgreSQL do .env
        self.db_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': os.getenv('POSTGRES_PORT', '5432'),
            'database': os.getenv('POSTGRES_DB', 'techchallenge03'),
            'user': os.getenv('POSTGRES_USER', 'admin'),
            'password': os.getenv('POSTGRES_PASSWORD', 'admin123')
        }
        
        # Caminho do arquivo CSV
        self.csv_path = Path('data/coffee_dataset/Coffe_sales.csv')
        
        # Nome da tabela no PostgreSQL
        self.table_name = 'coffee_sales'
        
        logger.info(f"Configura√ß√£o PostgreSQL: {self.db_config['user']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
    
    def create_connection_string(self) -> str:
        """
        Cria string de conex√£o para SQLAlchemy
        
        Returns:
            str: String de conex√£o PostgreSQL
        """
        return f"postgresql://{self.db_config['user']}:{self.db_config['password']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}"
    
    def test_connection(self) -> bool:
        """
        Testa a conex√£o com o PostgreSQL
        
        Returns:
            bool: True se conex√£o bem-sucedida
        """
        try:
            logger.info("üîå Testando conex√£o com PostgreSQL...")
            
            # Testar conex√£o direta com psycopg2
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            
            logger.info(f"‚úÖ Conex√£o bem-sucedida!")
            logger.info(f"   üìä PostgreSQL: {version}")
            
            cursor.close()
            conn.close()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na conex√£o: {str(e)}")
            logger.info("üí° Verifique se o PostgreSQL est√° rodando:")
            logger.info("   docker-compose up -d")
            logger.info("   ou ./manage-postgres.sh start")
            return False
    
    def load_csv_data(self) -> pd.DataFrame:
        """
        Carrega os dados do CSV
        
        Returns:
            pd.DataFrame: Dados do coffee sales
        """
        try:
            logger.info(f"üìÇ Carregando CSV: {self.csv_path}")
            
            if not self.csv_path.exists():
                raise FileNotFoundError(f"Arquivo CSV n√£o encontrado: {self.csv_path}")
            
            # Carregar CSV
            df = pd.read_csv(self.csv_path)
            logger.info(f"‚úÖ CSV carregado com sucesso!")
            logger.info(f"   üìè Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas")
            logger.info(f"   üìã Colunas: {list(df.columns)}")
            
            # Mostrar informa√ß√µes do dataset
            logger.info("üìà Informa√ß√µes do dataset:")
            logger.info(f"   ‚òï Total de vendas: {len(df):,}")
            
            if 'money' in df.columns:
                total_revenue = df['money'].sum()
                avg_sale = df['money'].mean()
                logger.info(f"   üí∞ Receita total: ${total_revenue:,.2f}")
                logger.info(f"   üìä Venda m√©dia: ${avg_sale:.2f}")
            
            if 'coffee_name' in df.columns:
                coffee_types = df['coffee_name'].nunique()
                logger.info(f"   ‚òï Tipos de caf√©: {coffee_types}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar CSV: {str(e)}")
            raise
    
    def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Prepara e limpa os dados para inser√ß√£o no PostgreSQL
        
        Args:
            df: DataFrame original
            
        Returns:
            pd.DataFrame: DataFrame preparado
        """
        try:
            logger.info("üîß Preparando dados para PostgreSQL...")
            
            # Criar c√≥pia para n√£o alterar o original
            df_clean = df.copy()
            
            # Converter colunas de data/hora se existirem
            if 'Date' in df_clean.columns:
                df_clean['Date'] = pd.to_datetime(df_clean['Date'])
                logger.info("   üìÖ Coluna 'Date' convertida para datetime")
            
            if 'Time' in df_clean.columns:
                # Analisar formatos de tempo presentes
                time_samples = df_clean['Time'].head(10).tolist()
                logger.info(f"   üîç Amostras de formato de tempo: {time_samples}")
                
                # Verificar se h√° microssegundos nos dados
                has_microseconds = any('.' in str(time_val) for time_val in time_samples if pd.notna(time_val))
                logger.info(f"   ‚è±Ô∏è  Microssegundos detectados: {has_microseconds}")
                
                try:
                    if has_microseconds:
                        # Converter usando formato inferred (pandas escolhe automaticamente)
                        df_clean['Time'] = pd.to_datetime(df_clean['Time'], infer_datetime_format=True).dt.time
                        logger.info("   ‚è∞ Coluna 'Time' convertida com infer√™ncia autom√°tica de formato")
                    else:
                        # Usar formato padr√£o sem microssegundos
                        df_clean['Time'] = pd.to_datetime(df_clean['Time'], format='%H:%M:%S').dt.time
                        logger.info("   ‚è∞ Coluna 'Time' convertida para time (formato padr√£o)")
                        
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è  Erro na convers√£o de tempo: {e}")
                    try:
                        # Fallback final: usar errors='coerce' para converter o que for poss√≠vel
                        df_clean['Time'] = pd.to_datetime(df_clean['Time'], errors='coerce').dt.time
                        invalid_count = df_clean['Time'].isna().sum()
                        valid_count = len(df_clean) - invalid_count
                        logger.info(f"   ‚úÖ Convers√£o com coer√ß√£o: {valid_count:,} v√°lidos, {invalid_count} inv√°lidos")
                        
                        if invalid_count > 0:
                            # Mostrar alguns exemplos de valores problem√°ticos
                            invalid_samples = df_clean[df_clean['Time'].isna()]['Time'].head(3)
                            logger.warning(f"   üö® Exemplos de valores problem√°ticos: {invalid_samples.tolist()}")
                            
                    except Exception as e2:
                        logger.error(f"   ‚ùå Falha total na convers√£o de tempo: {e2}")
                        # Manter como string se tudo falhar
                        logger.info("   üìù Mantendo coluna 'Time' como string")
            
            # Limpar nomes de colunas (remover espa√ßos, caracteres especiais)
            original_cols = df_clean.columns.tolist()
            df_clean.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_clean.columns]
            
            if original_cols != df_clean.columns.tolist():
                logger.info("   üî§ Nomes de colunas padronizados")
                for old, new in zip(original_cols, df_clean.columns):
                    if old != new:
                        logger.info(f"     '{old}' -> '{new}'")
            
            # Verificar valores nulos
            null_counts = df_clean.isnull().sum()
            if null_counts.sum() > 0:
                logger.warning("‚ö†Ô∏è  Valores nulos encontrados:")
                for col, count in null_counts[null_counts > 0].items():
                    logger.warning(f"     {col}: {count} nulos")
            
            logger.info(f"‚úÖ Dados preparados. Shape final: {df_clean.shape}")
            return df_clean
            
        except Exception as e:
            logger.error(f"‚ùå Erro na prepara√ß√£o dos dados: {str(e)}")
            raise
    
    def create_table(self, df: pd.DataFrame) -> bool:
        """
        Cria a tabela no PostgreSQL baseada na estrutura do DataFrame
        
        Args:
            df: DataFrame com os dados
            
        Returns:
            bool: True se tabela criada com sucesso
        """
        try:
            logger.info(f"üèóÔ∏è  Criando tabela '{self.table_name}' no PostgreSQL...")
            
            # Criar engine do SQLAlchemy
            engine = create_engine(self.create_connection_string())
            
            # Verificar se tabela j√° existe
            inspector = inspect(engine)
            if self.table_name in inspector.get_table_names():
                logger.warning(f"‚ö†Ô∏è  Tabela '{self.table_name}' j√° existe!")
                
                response = input("Deseja recriar a tabela? (s/N): ")
                if response.lower() in ['s', 'sim', 'y', 'yes']:
                    logger.info(f"üóëÔ∏è  Removendo tabela existente...")
                    with engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {self.table_name}"))
                        conn.commit()
                else:
                    logger.info("üìã Usando tabela existente")
                    return True
            
            # Usar pandas para criar a tabela automaticamente
            # Isso criar√° a tabela com tipos de dados apropriados
            sample_df = df.head(0)  # DataFrame vazio com s√≥ as colunas
            sample_df.to_sql(
                self.table_name,
                engine,
                if_exists='replace',
                index=False,
                method='multi'
            )
            
            logger.info(f"‚úÖ Tabela '{self.table_name}' criada com sucesso!")
            
            # Mostrar estrutura da tabela
            with engine.connect() as conn:
                result = conn.execute(text(f"""
                    SELECT column_name, data_type, is_nullable 
                    FROM information_schema.columns 
                    WHERE table_name = '{self.table_name}'
                    ORDER BY ordinal_position
                """))
                
                logger.info("üìã Estrutura da tabela:")
                for row in result:
                    logger.info(f"   {row[0]}: {row[1]} ({'NULL' if row[2] == 'YES' else 'NOT NULL'})")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar tabela: {str(e)}")
            return False
    
    def insert_data(self, df: pd.DataFrame) -> bool:
        """
        Insere os dados na tabela PostgreSQL
        
        Args:
            df: DataFrame com os dados preparados
            
        Returns:
            bool: True se inser√ß√£o bem-sucedida
        """
        try:
            logger.info(f"üì• Inserindo {len(df):,} registros na tabela '{self.table_name}'...")
            
            # Criar engine do SQLAlchemy
            engine = create_engine(self.create_connection_string())
            
            # Inserir dados em lotes para melhor performance
            batch_size = 1000
            total_batches = (len(df) + batch_size - 1) // batch_size
            
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                
                logger.info(f"   üì¶ Inserindo lote {batch_num}/{total_batches} ({len(batch)} registros)")
                
                batch.to_sql(
                    self.table_name,
                    engine,
                    if_exists='append',
                    index=False,
                    method='multi'
                )
            
            # Verificar total de registros inseridos
            with engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {self.table_name}"))
                count = result.scalar()
                
            logger.info(f"‚úÖ Inser√ß√£o conclu√≠da!")
            logger.info(f"   üìä Total de registros na tabela: {count:,}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na inser√ß√£o: {str(e)}")
            return False
    
    def verify_data(self) -> bool:
        """
        Verifica os dados inseridos na tabela
        
        Returns:
            bool: True se verifica√ß√£o bem-sucedida
        """
        try:
            logger.info(f"üîç Verificando dados na tabela '{self.table_name}'...")
            
            # Criar engine do SQLAlchemy
            engine = create_engine(self.create_connection_string())
            
            # Estat√≠sticas b√°sicas
            with engine.connect() as conn:
                # Contar registros
                count_result = conn.execute(text(f"SELECT COUNT(*) FROM {self.table_name}"))
                total_records = count_result.scalar()
                
                # Primeiros registros
                sample_result = conn.execute(text(f"SELECT * FROM {self.table_name} LIMIT 5"))
                sample_data = sample_result.fetchall()
                columns = sample_result.keys()
                
            logger.info(f"üìä Estat√≠sticas da tabela:")
            logger.info(f"   üìà Total de registros: {total_records:,}")
            logger.info(f"   üìã Colunas: {len(columns)}")
            
            # Mostrar amostra dos dados
            logger.info("üìã Amostra dos dados (5 primeiros registros):")
            for i, row in enumerate(sample_data, 1):
                logger.info(f"   {i}: {dict(zip(columns, row))}")
            
            # Verifica√ß√µes espec√≠ficas se colunas existirem
            with engine.connect() as conn:
                if 'money' in [col.lower() for col in columns]:
                    revenue_result = conn.execute(text(f"SELECT SUM(money)::numeric(10,2), AVG(money)::numeric(10,2) FROM {self.table_name}"))
                    total_rev, avg_rev = revenue_result.fetchone()
                    logger.info(f"   üí∞ Receita total: ${total_rev:,}")
                    logger.info(f"   üìä Venda m√©dia: ${avg_rev}")
                
                if 'coffee_name' in [col.lower() for col in columns]:
                    coffee_result = conn.execute(text(f"SELECT coffee_name, COUNT(*) FROM {self.table_name} GROUP BY coffee_name ORDER BY COUNT(*) DESC LIMIT 5"))
                    logger.info("   ‚òï Top 5 caf√©s mais vendidos:")
                    for coffee, count in coffee_result:
                        logger.info(f"     {coffee}: {count:,} vendas")
            
            logger.info("‚úÖ Verifica√ß√£o conclu√≠da com sucesso!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na verifica√ß√£o: {str(e)}")
            return False


def main():
    """Fun√ß√£o principal para executar o processo de persist√™ncia"""
    logger.info("=" * 70)
    logger.info("üöÄ TECH CHALLENGE 03 - PERSIST√äNCIA COFFEE SALES")
    logger.info("=" * 70)
    
    try:
        # Criar inst√¢ncia da classe de persist√™ncia
        persistencia = CoffeeSalesPersistencia()
        
        # 1. Testar conex√£o
        if not persistencia.test_connection():
            logger.error("‚ùå Falha na conex√£o com PostgreSQL")
            return 1
        
        # 2. Carregar dados do CSV
        df_original = persistencia.load_csv_data()
        
        # 3. Preparar dados
        df_clean = persistencia.prepare_data(df_original)
        
        # 4. Criar tabela
        if not persistencia.create_table(df_clean):
            logger.error("‚ùå Falha na cria√ß√£o da tabela")
            return 1
        
        # 5. Inserir dados
        if not persistencia.insert_data(df_clean):
            logger.error("‚ùå Falha na inser√ß√£o dos dados")
            return 1
        
        # 6. Verificar dados inseridos
        if not persistencia.verify_data():
            logger.error("‚ùå Falha na verifica√ß√£o dos dados")
            return 1
        
        logger.info("=" * 70)
        logger.info("‚úÖ PROCESSO DE PERSIST√äNCIA CONCLU√çDO COM SUCESSO!")
        logger.info("=" * 70)
        logger.info("üí° Para consultar os dados:")
        logger.info(f"   docker-compose exec postgres psql -U admin -d techchallenge03")
        logger.info(f"   SELECT * FROM coffee_sales LIMIT 10;")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("üõë Processo interrompido pelo usu√°rio")
        return 1
        
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado: {str(e)}")
        logger.exception("Detalhes do erro:")
        return 1


if __name__ == "__main__":
    """Ponto de entrada do script"""
    exit_code = main()
    sys.exit(exit_code)