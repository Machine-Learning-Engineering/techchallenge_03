# ğŸ—ï¸ Tech Challenge 03 - Data Warehouse Coffee Sales com LLM

Um projeto completo de anÃ¡lise de dados de vendas de cafÃ© utilizando Data Warehouse, API REST e tÃ©cnicas de LLM (Large Language Models) para insights inteligentes.

## ğŸ“ Estrutura do Projeto

```
techchallenge_03/
â”œâ”€â”€ ğŸ“‹ README.md                        # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ ğŸ“‹ SIMPLIFICACAO_DOCKER_COMPOSE.md  # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ ğŸ³ docker-compose.yml               # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ ğŸ³ Containerfile.api                # Containerfile para API
â”œâ”€â”€ ğŸ³ Containerfile.postgres           # Containerfile para PostgreSQL
â”œâ”€â”€ ğŸš€ start-api.sh                     # Script de inicializaÃ§Ã£o
â””â”€â”€ ğŸ“ src/                             # CÃ³digo fonte
    â”œâ”€â”€ ğŸ“Š analise_dw_coffee_llm.ipynb  # Notebook principal de anÃ¡lise
    â”œâ”€â”€ ğŸš€ api.py                       # API REST FastAPI
    â”œâ”€â”€ ğŸ“¥ data_upload.py               # Download de datasets
    â”œâ”€â”€ ğŸ’¾ persistencia.py              # PersistÃªncia no PostgreSQL
    â”œâ”€â”€ ğŸ—ï¸ dw_tratamento.py             # Tratamento e criaÃ§Ã£o do DW
    â”œâ”€â”€ ğŸ“‹ requirements.txt             # DependÃªncias Python
```

## ğŸ› ï¸ Tecnologias e Bibliotecas Utilizadas

### Backend e API
- **FastAPI** ğŸš€ - Framework web moderno para APIs
- **Uvicorn** âš¡ - Servidor ASGI de alta performance
- **Pydantic** ğŸ“‹ - ValidaÃ§Ã£o de dados

### Banco de Dados
- **PostgreSQL 15** ğŸ˜ - Banco de dados relacional
- **SQLAlchemy** ğŸ”§ - ORM Python
- **psycopg2-binary** ğŸ”Œ - Driver PostgreSQL

### AnÃ¡lise de Dados
- **Pandas** ğŸ¼ - ManipulaÃ§Ã£o e anÃ¡lise de dados
- **NumPy** ğŸ”¢ - ComputaÃ§Ã£o numÃ©rica
- **Matplotlib** ğŸ“Š - VisualizaÃ§Ãµes bÃ¡sicas
- **Seaborn** ğŸ¨ - VisualizaÃ§Ãµes estatÃ­sticas
- **Plotly** ğŸ“ˆ - GrÃ¡ficos interativos

### Ambiente de Desenvolvimento
- **Jupyter Notebook** ğŸ““ - Ambiente interativo
- **Python 3.11** ğŸ - Linguagem de programaÃ§Ã£o
- **Podman** ğŸ³ - ContainerizaÃ§Ã£o

### Outras DependÃªncias
- **KaggleHub** ğŸ“¦ - Download de datasets
- **python-dotenv** âš™ï¸ - VariÃ¡veis de ambiente
- **Kaleido** ğŸ–¼ï¸ - ExportaÃ§Ã£o de grÃ¡ficos Plotly

## ğŸ”§ Requisitos para ExecuÃ§Ã£o

### PrÃ©-requisitos
- **Podman** instalado
- **Podman Compose** instalado
- **8GB RAM** recomendado
- **2GB** de espaÃ§o em disco

### Verificar InstalaÃ§Ã£o
```bash
# Verificar Podman
podman --version

# Verificar Podman Compose
podman-compose --version
```

## ğŸš€ Como Executar a AplicaÃ§Ã£o

### 1. Clone o RepositÃ³rio
```bash
git clone https://github.com/Machine-Learning-Engineering/techchallenge_03.git
cd techchallenge_03
```

### 2. Iniciar a AplicaÃ§Ã£o Completa
```bash
# Com Podman
podman-compose up --build --no-cache
```

### 3. Aguardar InicializaÃ§Ã£o
O sistema inicializarÃ¡ na seguinte ordem:
1. **PostgreSQL** (porta 5432) ğŸ˜
2. **InicializaÃ§Ã£o do banco** (criaÃ§Ã£o de estruturas)
3. **API FastAPI** (porta 8000) ğŸš€
4. **Jupyter Notebook** (porta 8888) ğŸ““

### 4. Verificar ServiÃ§os
```bash
# Verificar containers em execuÃ§Ã£o
podman ps
```

## ğŸŒ Como Executar as APIs

### DocumentaÃ§Ã£o Swagger
Acesse a documentaÃ§Ã£o interativa da API:
- **URL**: http://localhost:8000/docs
- **Alternativa ReDoc**: http://localhost:8000/redoc

### Endpoints Principais

#### 1. Pipeline Completo (Recomendado)
```bash
# Executar todo o pipeline automaticamente
curl -X POST "http://localhost:8000/pipeline/execute" \
     -H "Content-Type: application/json"
```

#### 2. ExecuÃ§Ã£o Passo a Passo

**Download dos Dados:**
```bash
curl -X POST "http://localhost:8000/data-upload/" \
     -H "Content-Type: application/json" \
     -d '{"force_download": false}'
```

**PersistÃªncia no PostgreSQL:**
```bash
curl -X POST "http://localhost:8000/persistencia/" \
     -H "Content-Type: application/json" \
     -d '{"force_reload": false}'
```

**CriaÃ§Ã£o do Data Warehouse:**
```bash
curl -X POST "http://localhost:8000/dw-tratamento/" \
     -H "Content-Type: application/json" \
     -d '{"force_rebuild": false}'
```

#### 3. VerificaÃ§Ã£o de Status
```bash
# Status da API
curl -X GET "http://localhost:8000/"

# SaÃºde da aplicaÃ§Ã£o
curl -X GET "http://localhost:8000/health"
```

## ğŸ““ Como Executar o Notebook

### 1. Acessar Jupyter Notebook
Abra seu navegador em: **http://localhost:8888**

### 2. Abrir o Notebook Principal
Navegue atÃ©: `src/analise_dw_coffee_llm.ipynb`

### 3. Executar AnÃ¡lise
1. **Execute todas as cÃ©lulas** usando `Cell â†’ Run All`
2. **Ou execute passo a passo** usando `Shift + Enter`

### 4. Requisitos Importantes
- **Certifique-se** que o pipeline da API foi executado primeiro
- **Verifique** se a tabela `dw_coffee` existe no PostgreSQL
- **Aguarde** o carregamento completo dos dados (3.547 registros)

## ğŸ¤– Algoritmos e TÃ©cnicas de LLM Utilizadas

### 1. AnÃ¡lise SemÃ¢ntica de PadrÃµes
- **TÃ©cnica**: Processamento de linguagem natural aplicado aos dados
- **Objetivo**: Identificar padrÃµes ocultos nas vendas de cafÃ©
- **ImplementaÃ§Ã£o**: AnÃ¡lise de correlaÃ§Ãµes usando algoritmos de clustering semÃ¢ntico

### 2. SimulaÃ§Ã£o de LLM para Insights
- **TÃ©cnica**: GeraÃ§Ã£o automÃ¡tica de insights baseada em regras inteligentes
- **Algoritmo**: Sistema de recomendaÃ§Ã£o baseado em padrÃµes temporais
- **AplicaÃ§Ã£o**: AnÃ¡lise de tendÃªncias de vendas por perÃ­odo e categoria

### 3. AnÃ¡lise Preditiva Baseada em Contexto
- **TÃ©cnica**: Algoritmos de anÃ¡lise temporal com contexto semÃ¢ntico
- **ImplementaÃ§Ã£o**: Uso de sÃ©ries temporais com anÃ¡lise de sentimento simulada
- **Objetivo**: Prever comportamentos de compra baseados em padrÃµes histÃ³ricos

### 4. Processamento de Texto Estruturado
- **Algoritmo**: NLP aplicado aos nomes de produtos e categorias
- **TÃ©cnica**: TokenizaÃ§Ã£o e anÃ¡lise de frequÃªncia
- **Uso**: ExtraÃ§Ã£o de insights sobre preferÃªncias de produtos

### 5. Clustering Inteligente de Clientes
- **TÃ©cnica**: SegmentaÃ§Ã£o baseada em comportamento de compra
- **Algoritmo**: K-means adaptado com pesos semÃ¢nticos
- **Objetivo**: Identificar grupos de clientes com padrÃµes similares

### 6. AnÃ¡lise de Sentimento Simulada
- **TÃ©cnica**: ClassificaÃ§Ã£o de produtos por popularidade e sazonalidade
- **ImplementaÃ§Ã£o**: Sistema de pontuaÃ§Ã£o baseado em vendas e frequÃªncia
- **AplicaÃ§Ã£o**: Ranking de produtos mais "desejados" pelo mercado

## ğŸ“š ReferÃªncias dos Algoritmos

### AnÃ¡lise de Dados e Data Science
1. **McKinney, W.** (2022). *Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter*. O'Reilly Media.

2. **VanderPlas, J.** (2016). *Python Data Science Handbook: Essential Tools for Working with Data*. O'Reilly Media.

### TÃ©cnicas de LLM e NLP
3. **Jurafsky, D., & Martin, J. H.** (2023). *Speech and Language Processing*. 3rd Edition. Pearson.

4. **Khurana, D., et al.** (2023). "Natural Language Processing: State of The Art, Current Trends and Challenges." *Multimedia Tools and Applications*, 82(3), 3713-3744.

### Machine Learning e Clustering
5. **Hastie, T., Tibshirani, R., & Friedman, J.** (2017). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. 2nd Edition. Springer.

6. **Scikit-learn Developers** (2023). *Scikit-learn: Machine Learning in Python*. Journal of Machine Learning Research.

### VisualizaÃ§Ã£o de Dados
7. **Cairo, A.** (2019). *How Charts Lie: Getting Smarter about Visual Information*. W. W. Norton & Company.

8. **Plotly Technologies Inc.** (2023). *Plotly Python Graphing Library Documentation*. https://plotly.com/python/

### Data Warehousing
9. **Kimball, R., & Ross, M.** (2013). *The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling*. 3rd Edition. Wiley.

10. **Inmon, W. H.** (2005). *Building the Data Warehouse*. 4th Edition. Wiley.

### APIs e Desenvolvimento Web
11. **FastAPI Documentation** (2023). *FastAPI: Modern, fast (high-performance), web framework for building APIs*. https://fastapi.tiangolo.com/

12. **PostgreSQL Global Development Group** (2023). *PostgreSQL Documentation*. https://www.postgresql.org/docs/

---

## ğŸ¯ Funcionalidades Principais

- âœ… **Pipeline ETL Completo** - ExtraÃ§Ã£o, transformaÃ§Ã£o e carregamento automÃ¡tico
- âœ… **Data Warehouse** - Estrutura otimizada para anÃ¡lise
- âœ… **API REST** - Endpoints para executar o pipeline
- âœ… **AnÃ¡lise com LLM** - Insights inteligentes automatizados  
- âœ… **VisualizaÃ§Ãµes Interativas** - GrÃ¡ficos Plotly responsivos
- âœ… **ContainerizaÃ§Ã£o** - Deploy fÃ¡cil com Podman
- âœ… **DocumentaÃ§Ã£o Swagger** - API autodocumentada
- âœ… **Jupyter Notebook** - Ambiente interativo de anÃ¡lise

## ğŸ“Š Dados do Projeto

- **Dataset Original**: [Coffee Sales - Kaggle](https://www.kaggle.com/datasets/minahilfatima12328/daily-coffee-transactions/data)
- **Registros**: 3.547 vendas processadas
- **Tabelas**: `coffee_sales` (raw) â†’ `dw_coffee` (processada)
- **PerÃ­odo**: Dados histÃ³ricos de vendas de cafÃ©
- **Qualidade**: Dados limpos, sem outliers e valores ausentes

## ğŸ†˜ Suporte e Troubleshooting

### Problemas Comuns

1. **Erro de conexÃ£o com PostgreSQL**: 
   - Aguarde a inicializaÃ§Ã£o completa dos containers
   - Verifique se a porta 5432 nÃ£o estÃ¡ em uso

2. **Notebook nÃ£o carrega dados**:
   - Execute o pipeline da API primeiro
   - Verifique se a tabela `dw_coffee` existe

3. **API nÃ£o responde**:
   - Verifique se o container estÃ¡ rodando: `podman ps`
   - Consulte os logs: `podman logs coffee-api`

### Logs do Sistema
```bash
# Ver logs da API
podman logs coffee-api

# Ver logs do PostgreSQL  
podman logs techchallenge03-postgres

# Ver logs em tempo real
podman logs -f coffee-api
```

---

**Desenvolvido para Tech Challenge 03** ğŸš€  
*Data Warehouse, APIs e LLM para anÃ¡lise inteligente de dados*

